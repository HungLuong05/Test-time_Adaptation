{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as torchF\n",
    "import torch.jit\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms.functional as torchvisionF\n",
    "from torchvision.transforms import ColorJitter, Compose, Lambda\n",
    "from numpy import random\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "#from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from time import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet18 on ImageNet-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\duchu/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\Users\\duchu\\CodeHub\\VietAI\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\duchu\\CodeHub\\VietAI\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ResNet model implementation\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.fc = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced data experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = \"./data/Tiny-ImageNet-C\"\n",
    "imbalanced_data_folders = [\"brightness\", \"contrast\", \"defocus_blur\", \"elastic_transform\", \"fog\", \"frost\", \"gaussian_noise\", \"glass_blur\", \"impulse_noise\", \"jpeg_compression\", \"motion_blur\", \"pixelate\", \"shot_noise\", \"snow\", \"zoom_blur\"]\n",
    "dir_mapping = \"mapping.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "with open(os.path.join(dir_data, dir_mapping)) as mapping_file:\n",
    "    num = 0\n",
    "    for data in mapping_file:\n",
    "        file_name = data.split(' ')[0]\n",
    "        mapping[file_name] = num\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(dir_image):\n",
    "    image = Image.open(dir_image)\n",
    "    image.show()\n",
    "\n",
    "def get_accuracy(outputs, ground_truth):\n",
    "    num = len(outputs)\n",
    "    sum = torch.sum(torch.tensor(outputs) == torch.tensor(ground_truth))\n",
    "    return sum / num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 4184.18it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3733.31it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1682.52it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3540.03it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4425.16it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4898.57it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4085.13it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4234.85it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 2584.39it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4074.85it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3982.34it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4447.64it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1557.05it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3361.44it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4052.88it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 5137.97it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4776.05it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3787.64it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4221.17it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3972.18it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4104.68it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4602.22it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4077.37it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1599.89it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3003.34it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3971.16it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4969.17it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 5214.36it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3773.55it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 5298.98it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3875.16it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4471.85it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4700.95it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4512.94it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3994.46it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4504.12it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1498.88it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4170.47it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 5239.64it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4430.54it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3990.30it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4106.79it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4607.83it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3594.28it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3057.82it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3764.12it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 5130.24it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 5039.05it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4112.37it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4374.35it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4056.56it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3163.69it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3449.14it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1446.70it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3266.76it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3496.46it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 5257.67it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 5342.35it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3656.58it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4163.95it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3456.77it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3952.19it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4417.94it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4322.51it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4007.90it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4514.01it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4766.06it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3991.82it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4454.23it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4178.61it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4890.55it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4304.52it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3908.77it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4115.07it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4308.55it/s]\n"
     ]
    }
   ],
   "source": [
    "imagenet_dir_data = \"./data/Tiny-ImageNet-C\"\n",
    "imagenet_imbalanced_data_folders = [\"brightness\", \"contrast\", \"defocus_blur\", \"elastic_transform\", \"fog\", \"frost\", \"gaussian_noise\", \"glass_blur\", \"impulse_noise\", \"jpeg_compression\", \"motion_blur\", \"pixelate\", \"shot_noise\", \"snow\", \"zoom_blur\"]\n",
    "imagenet_dir_mapping = \"mapping.txt\"\n",
    "\n",
    "all_imagenet = []\n",
    "for imbalanced_data_folder in imbalanced_data_folders:\n",
    "    for index in range(1, 6):\n",
    "        dir_corrupt = os.path.join(dir_data, imbalanced_data_folder, str(index))\n",
    "        dir_classes = os.listdir(dir_corrupt)\n",
    "        for dir_class in tqdm(dir_classes):\n",
    "            data_folder = os.path.join(dir_corrupt, dir_class)\n",
    "            label = mapping[dir_class]\n",
    "            images = os.listdir(data_folder)\n",
    "            for image in images:\n",
    "                dir_image = os.path.join(data_folder, image)\n",
    "                all_imagenet.append([dir_image, label])\n",
    "random.shuffle(all_imagenet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [03:29<00:00,  1.05s/it]\n",
      "100%|██████████| 200/200 [03:34<00:00,  1.07s/it]\n",
      "100%|██████████| 200/200 [03:35<00:00,  1.08s/it]\n",
      "100%|██████████| 200/200 [03:34<00:00,  1.07s/it]\n",
      "100%|██████████| 200/200 [03:35<00:00,  1.08s/it]\n",
      "100%|██████████| 200/200 [03:36<00:00,  1.08s/it]\n",
      "100%|██████████| 200/200 [03:40<00:00,  1.10s/it]\n",
      "100%|██████████| 200/200 [03:39<00:00,  1.10s/it]\n",
      "100%|██████████| 200/200 [03:40<00:00,  1.10s/it]\n",
      "100%|██████████| 200/200 [03:38<00:00,  1.09s/it]\n",
      "100%|██████████| 200/200 [03:39<00:00,  1.10s/it]\n",
      "100%|██████████| 200/200 [03:40<00:00,  1.10s/it]\n",
      "100%|██████████| 200/200 [03:41<00:00,  1.11s/it]\n",
      "100%|██████████| 200/200 [03:39<00:00,  1.10s/it]\n",
      "100%|██████████| 200/200 [03:41<00:00,  1.11s/it]\n",
      "100%|██████████| 200/200 [03:41<00:00,  1.11s/it]\n",
      "100%|██████████| 200/200 [03:40<00:00,  1.10s/it]\n",
      "100%|██████████| 200/200 [03:41<00:00,  1.11s/it]\n",
      "100%|██████████| 200/200 [03:41<00:00,  1.11s/it]\n",
      "100%|██████████| 200/200 [03:40<00:00,  1.10s/it]\n",
      "100%|██████████| 200/200 [03:41<00:00,  1.11s/it]\n",
      "100%|██████████| 200/200 [03:40<00:00,  1.10s/it]\n",
      "100%|██████████| 200/200 [03:41<00:00,  1.11s/it]\n",
      "100%|██████████| 200/200 [03:41<00:00,  1.11s/it]\n",
      "100%|██████████| 200/200 [03:41<00:00,  1.11s/it]\n",
      "100%|██████████| 200/200 [03:40<00:00,  1.10s/it]\n",
      "100%|██████████| 200/200 [03:34<00:00,  1.07s/it]\n",
      "100%|██████████| 200/200 [03:35<00:00,  1.08s/it]\n",
      "100%|██████████| 200/200 [03:36<00:00,  1.08s/it]\n",
      "100%|██████████| 200/200 [03:38<00:00,  1.09s/it]\n",
      "100%|██████████| 200/200 [03:39<00:00,  1.10s/it]\n",
      "100%|██████████| 200/200 [03:40<00:00,  1.10s/it]\n",
      "100%|██████████| 200/200 [03:40<00:00,  1.10s/it]\n",
      "100%|██████████| 200/200 [03:40<00:00,  1.10s/it]\n",
      "100%|██████████| 200/200 [03:41<00:00,  1.11s/it]\n",
      "100%|██████████| 200/200 [03:39<00:00,  1.10s/it]\n",
      "100%|██████████| 200/200 [03:39<00:00,  1.10s/it]\n",
      "100%|██████████| 200/200 [03:40<00:00,  1.10s/it]\n",
      "100%|██████████| 200/200 [03:41<00:00,  1.11s/it]\n",
      "100%|██████████| 200/200 [03:39<00:00,  1.10s/it]\n",
      "100%|██████████| 200/200 [03:40<00:00,  1.10s/it]\n",
      "100%|██████████| 200/200 [04:02<00:00,  1.21s/it]\n",
      "100%|██████████| 200/200 [04:54<00:00,  1.47s/it]\n",
      "100%|██████████| 200/200 [05:09<00:00,  1.55s/it]\n",
      "100%|██████████| 200/200 [04:58<00:00,  1.49s/it]\n",
      "100%|██████████| 200/200 [04:55<00:00,  1.48s/it]\n",
      "100%|██████████| 200/200 [04:52<00:00,  1.46s/it]\n",
      "100%|██████████| 200/200 [04:51<00:00,  1.46s/it]\n",
      "100%|██████████| 200/200 [03:53<00:00,  1.17s/it]\n",
      "100%|██████████| 200/200 [04:24<00:00,  1.32s/it]\n",
      "100%|██████████| 200/200 [05:04<00:00,  1.52s/it]\n",
      "100%|██████████| 200/200 [05:17<00:00,  1.59s/it]\n",
      "100%|██████████| 200/200 [06:02<00:00,  1.81s/it]\n",
      "100%|██████████| 200/200 [11:38<00:00,  3.49s/it]\n",
      "100%|██████████| 200/200 [13:09<00:00,  3.95s/it]\n",
      "100%|██████████| 200/200 [12:36<00:00,  3.78s/it]\n",
      "100%|██████████| 200/200 [12:41<00:00,  3.81s/it]\n",
      "100%|██████████| 200/200 [10:20<00:00,  3.10s/it]\n",
      "100%|██████████| 200/200 [15:09<00:00,  4.55s/it]\n",
      "100%|██████████| 200/200 [13:33<00:00,  4.07s/it]\n",
      "100%|██████████| 200/200 [13:22<00:00,  4.01s/it]\n",
      "100%|██████████| 200/200 [15:44<00:00,  4.72s/it]\n",
      "100%|██████████| 200/200 [16:33<00:00,  4.97s/it]\n",
      "100%|██████████| 200/200 [16:21<00:00,  4.91s/it]\n",
      "100%|██████████| 200/200 [41:58<00:00, 12.59s/it]   \n",
      "100%|██████████| 200/200 [05:54<00:00,  1.77s/it]\n",
      "100%|██████████| 200/200 [04:07<00:00,  1.24s/it]\n",
      "100%|██████████| 200/200 [04:06<00:00,  1.23s/it]\n",
      "100%|██████████| 200/200 [04:04<00:00,  1.22s/it]\n",
      "100%|██████████| 200/200 [04:02<00:00,  1.21s/it]\n",
      "100%|██████████| 200/200 [04:03<00:00,  1.22s/it]\n",
      "100%|██████████| 200/200 [04:11<00:00,  1.26s/it]\n",
      "100%|██████████| 200/200 [04:31<00:00,  1.36s/it]\n",
      "100%|██████████| 200/200 [07:25<00:00,  2.23s/it]\n",
      "100%|██████████| 200/200 [07:32<00:00,  2.26s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Evaluate the model on the corrupted data without using test-time adaptation technique\n",
    "\"\"\"\n",
    "\n",
    "outputs = []\n",
    "ground_truth = []\n",
    "for imbalanced_data_folder in imbalanced_data_folders:\n",
    "    for index in range(1, 6):\n",
    "        dir_corrupt = os.path.join(dir_data, imbalanced_data_folder)\n",
    "        dir_corrupt = os.path.join(dir_corrupt, str(index))\n",
    "        dir_classes = os.listdir(dir_corrupt)\n",
    "        for dir_class in tqdm.tqdm(dir_classes):\n",
    "            data_folder = os.path.join(dir_corrupt, dir_class)\n",
    "            label = mapping[dir_class]\n",
    "            images = os.listdir(data_folder)\n",
    "            for image in images:\n",
    "                dir_image = os.path.join(data_folder, image)\n",
    "                image = Image.open(dir_image)\n",
    "\n",
    "                input = preprocess(image).unsqueeze(0)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output = model(input)\n",
    "\n",
    "                    ground_truth.append(label)\n",
    "                    outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(outputs)):\n",
    "    outputs[i] = np.argmax(outputs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0988)\n"
     ]
    }
   ],
   "source": [
    "accuracy = get_accuracy(outputs, ground_truth)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoTTA (Continual Test-time Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TENT (Test-time Adaptation by Entropy Minimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Builds upon: https://github.com/qinenergy/cotta\n",
    "Corresponding paper: https://arxiv.org/abs/2006.10726\n",
    "\"\"\"\n",
    "\n",
    "class Tent(nn.Module):\n",
    "    \"\"\"\n",
    "    Tent adapts a model by entropy minimization during testing.\n",
    "    Once tented, a model adapts itself by updating on every forward.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, optimizer, steps = 1):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.steps = steps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for _ in range(self.steps):\n",
    "            outputs = tent_forward_and_adapt(x, self.model, self.optimizer)\n",
    "        return outputs\n",
    "\n",
    "@torch.jit.script\n",
    "def tent_softmax_entropy(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Entropy of softmax distribution from logits.\"\"\"\n",
    "    return -(x.softmax(1) * x.log_softmax(1)).sum(1)\n",
    "\n",
    "\n",
    "@torch.enable_grad()  # ensure grads in possible no grad context for testing\n",
    "def tent_forward_and_adapt(x, model, optimizer):\n",
    "    \"\"\"Forward and adapt model on batch of data.\n",
    "\n",
    "    Measure entropy of the model prediction, take gradients, and update params.\n",
    "    \"\"\"\n",
    "    # forward\n",
    "    outputs = model(x)\n",
    "    # adapt\n",
    "    loss = tent_softmax_entropy(outputs).mean(0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return outputs\n",
    "\n",
    "def tent_configure_model(model):\n",
    "    \"\"\"Configure model for use with tent.\"\"\"\n",
    "    # train mode, because tent optimizes the model to minimize entropy\n",
    "    model.train()\n",
    "    # disable grad, to (re-)enable only what tent updates\n",
    "    model.requires_grad_(False)\n",
    "    # configure norm for tent updates: enable grad + force batch statisics\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            m.requires_grad_(True)\n",
    "            # force use of batch stats in train and eval modes\n",
    "            m.track_running_stats = False\n",
    "            m.running_mean = None\n",
    "            m.running_var = None\n",
    "    return model\n",
    "\n",
    "def tent_collect_params(model):\n",
    "    \"\"\"Collect the affine scale + shift parameters from batch norms.\n",
    "\n",
    "    Walk the model's modules and collect all batch normalization parameters.\n",
    "    Return the parameters and their names.\n",
    "\n",
    "    Note: other choices of parameterization are possible!\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    names = []\n",
    "    for nm, m in model.named_modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            for np, p in m.named_parameters():\n",
    "                if np in ['weight', 'bias']:  # weight is scale, bias is shift\n",
    "                    params.append(p)\n",
    "                    names.append(f\"{nm}.{np}\")\n",
    "    return params, names\n",
    "\n",
    "def tent_check_model(model):\n",
    "    \"\"\"Check model for compatability with tent.\"\"\"\n",
    "    is_training = model.training\n",
    "    assert is_training, \"tent needs train mode: call model.train()\"\n",
    "    param_grads = [p.requires_grad for p in model.parameters()]\n",
    "    has_any_params = any(param_grads)\n",
    "    has_all_params = all(param_grads)\n",
    "    assert has_any_params, \"tent needs params to update: \" \\\n",
    "                           \"check which require grad\"\n",
    "    assert not has_all_params, \"tent should not update all params: \" \\\n",
    "                               \"check which require grad\"\n",
    "    has_bn = any([isinstance(m, nn.BatchNorm2d) for m in model.modules()])\n",
    "    assert has_bn, \"tent needs normalization for its optimization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_raw(model):\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def setup_tent(model, steps = 1):\n",
    "    \"\"\"\n",
    "    Set up tent adaptation.\n",
    "    \"\"\"\n",
    "    model = tent_configure_model(model)\n",
    "    params, param_names = tent_collect_params(model)\n",
    "    optimizer = optim.Adam(params = params)\n",
    "    tent_model = Tent(model = model, optimizer = optimizer, steps = steps)\n",
    "    return tent_model\n",
    "\n",
    "def setup_cotta(model, steps = 1):\n",
    "    \"\"\"\n",
    "    Set up CoTTA adaptation.\n",
    "    \"\"\"\n",
    "    model = cotta_configure_model(model)\n",
    "    params, param_names = cotta_collect_params(model)\n",
    "    optimizer = optim.Adam(params = params, lr = 0.01)\n",
    "    cotta_model = CoTTA(model = model, optimizer = optimizer, steps = steps)\n",
    "    return cotta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tiny_imagenet_C(model, batch_size = 100):\n",
    "    # evaluate on each severity and type of corruption in turn\n",
    "    outputs = torch.tensor([])\n",
    "    ground_truth = []\n",
    "    images = torch.tensor([])\n",
    "\n",
    "    while(batch_size):\n",
    "        dir_image, label = all_imagenet[random.randint(len(all_imagenet))]\n",
    "        image = Image.open(dir_image)\n",
    "        input = preprocess(image).unsqueeze(0)\n",
    "        if input.numel() == 0:\n",
    "            images = input\n",
    "        else:\n",
    "            images = torch.cat((images, input))\n",
    "        \n",
    "        ground_truth.append(label)\n",
    "        batch_size -= 1\n",
    "    \n",
    "    images = torch.tensor(images)\n",
    "    with torch.no_grad():\n",
    "        output = model.forward(images)\n",
    "        outputs = np.argmax(output, axis = 1)\n",
    "    \n",
    "    #print(outputs)\n",
    "    #print(ground_truth)\n",
    "    accuracy = get_accuracy(outputs, ground_truth)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duchu\\AppData\\Local\\Temp\\ipykernel_17060\\444655571.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  images = torch.tensor(images)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0800)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duchu\\AppData\\Local\\Temp\\ipykernel_17060\\296248698.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sum = torch.sum(torch.tensor(outputs) == torch.tensor(ground_truth))\n"
     ]
    }
   ],
   "source": [
    "tent_model = setup_tent(model)\n",
    "tent_check_model(tent_model)\n",
    "accuracy = evaluate_tiny_imagenet_C(tent_model)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duchu\\AppData\\Local\\Temp\\ipykernel_17060\\444655571.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  images = torch.tensor(images)\n",
      "C:\\Users\\duchu\\AppData\\Local\\Temp\\ipykernel_17060\\296248698.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sum = torch.sum(torch.tensor(outputs) == torch.tensor(ground_truth))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0600)\n",
      "tensor(0.0800)\n",
      "tensor(0.1100)\n",
      "tensor(0.1000)\n",
      "tensor(0.0500)\n",
      "tensor(0.0600)\n",
      "tensor(0.0700)\n",
      "tensor(0.1500)\n",
      "tensor(0.0700)\n",
      "tensor(0.0200)\n",
      "tensor(0.0900)\n",
      "tensor(0.0400)\n",
      "tensor(0.1000)\n",
      "tensor(0.0900)\n",
      "tensor(0.0700)\n",
      "tensor(0.0700)\n",
      "tensor(0.0900)\n",
      "tensor(0.1000)\n",
      "tensor(0.0700)\n",
      "tensor(0.1200)\n",
      "tensor(0.0900)\n",
      "tensor(0.0600)\n",
      "tensor(0.0600)\n",
      "tensor(0.1200)\n",
      "tensor(0.0700)\n",
      "tensor(0.0800)\n",
      "tensor(0.0800)\n",
      "tensor(0.1300)\n",
      "tensor(0.1000)\n",
      "tensor(0.0500)\n",
      "tensor(0.0700)\n",
      "tensor(0.0500)\n",
      "tensor(0.1400)\n",
      "tensor(0.0800)\n",
      "tensor(0.0300)\n",
      "tensor(0.0700)\n",
      "tensor(0.0700)\n",
      "tensor(0.0600)\n",
      "tensor(0.0500)\n",
      "tensor(0.0900)\n",
      "tensor(0.0800)\n",
      "tensor(0.0500)\n",
      "tensor(0.0200)\n",
      "tensor(0.0500)\n",
      "tensor(0.0600)\n",
      "tensor(0.0200)\n",
      "tensor(0.0700)\n",
      "tensor(0.0600)\n",
      "tensor(0.0500)\n",
      "tensor(0.0700)\n",
      "tensor(0.0500)\n",
      "tensor(0.0500)\n",
      "tensor(0.0500)\n",
      "tensor(0.0700)\n",
      "tensor(0.0900)\n",
      "tensor(0.0600)\n",
      "tensor(0.0800)\n",
      "tensor(0.0700)\n",
      "tensor(0.0700)\n",
      "tensor(0.0900)\n",
      "tensor(0.0200)\n",
      "tensor(0.0200)\n",
      "tensor(0.0300)\n",
      "tensor(0.0700)\n",
      "tensor(0.0600)\n",
      "tensor(0.0500)\n",
      "tensor(0.0400)\n",
      "tensor(0.0400)\n",
      "tensor(0.0500)\n",
      "tensor(0.0300)\n",
      "tensor(0.0300)\n",
      "tensor(0.0200)\n",
      "tensor(0.0500)\n",
      "tensor(0.0400)\n",
      "tensor(0.0500)\n",
      "tensor(0.0300)\n",
      "tensor(0.0400)\n",
      "tensor(0.0500)\n",
      "tensor(0.0400)\n",
      "tensor(0.0200)\n",
      "tensor(0.0400)\n",
      "tensor(0.0700)\n",
      "tensor(0.0600)\n",
      "tensor(0.0200)\n",
      "tensor(0.0100)\n",
      "tensor(0.0300)\n",
      "tensor(0.)\n",
      "tensor(0.0300)\n",
      "tensor(0.0500)\n",
      "tensor(0.0400)\n",
      "tensor(0.0500)\n",
      "tensor(0.0200)\n",
      "tensor(0.0400)\n",
      "tensor(0.0600)\n",
      "tensor(0.0300)\n",
      "tensor(0.0100)\n",
      "tensor(0.0300)\n",
      "tensor(0.0200)\n",
      "tensor(0.0100)\n",
      "tensor(0.0600)\n",
      "tensor(0.0600)\n"
     ]
    }
   ],
   "source": [
    "tent_model = setup_tent(model, steps = 1)\n",
    "tent_check_model(tent_model)\n",
    "for epoch in range(100):\n",
    "    accuracy = evaluate_tiny_imagenet_C(tent_model)\n",
    "    print(accuracy)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
