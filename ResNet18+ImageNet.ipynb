{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as torchF\n",
    "import torch.jit\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms.functional as torchvisionF\n",
    "from torchvision.transforms import ColorJitter, Compose, Lambda\n",
    "from numpy import random\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "#from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from time import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet18 on ImageNet-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\duchu/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\Users\\duchu\\CodeHub\\VietAI\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\duchu\\CodeHub\\VietAI\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in C:\\Users\\duchu/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "Using cache found in C:\\Users\\duchu/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)\n",
    "model1.eval()\n",
    "model2 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "model2.eval()\n",
    "model3 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "model3.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced data experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = \"./data/Tiny-ImageNet-C\"\n",
    "imbalanced_data_folders = [\"brightness\", \"contrast\", \"defocus_blur\", \"elastic_transform\", \"fog\", \"frost\", \"gaussian_noise\", \"glass_blur\", \"impulse_noise\", \"jpeg_compression\", \"motion_blur\", \"pixelate\", \"shot_noise\", \"snow\", \"zoom_blur\"]\n",
    "dir_mapping = \"mapping.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "with open(os.path.join(dir_data, dir_mapping)) as mapping_file:\n",
    "    num = 0\n",
    "    for data in mapping_file:\n",
    "        file_name = data.split(' ')[0]\n",
    "        mapping[file_name] = num\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(dir_image):\n",
    "    image = Image.open(dir_image)\n",
    "    image.show()\n",
    "\n",
    "def get_accuracy(outputs, ground_truth):\n",
    "    num = len(outputs)\n",
    "    sum = np.sum(np.array(outputs) == np.array(ground_truth))\n",
    "    return sum / num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_dir_data = \"./data/Tiny-ImageNet-C\"\n",
    "imagenet_imbalanced_data_folders = [\"brightness\", \"contrast\", \"defocus_blur\", \"elastic_transform\", \"fog\", \"frost\", \"gaussian_noise\", \"glass_blur\", \"impulse_noise\", \"jpeg_compression\", \"motion_blur\", \"pixelate\", \"shot_noise\", \"snow\", \"zoom_blur\"]\n",
    "imagenet_dir_mapping = \"mapping.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoTTA (Continual Test-time Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoise(torch.nn.Module):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        super().__init__()\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "\n",
    "    def forward(self, img):\n",
    "        noise = torch.randn(img.size()) * self.std + self.mean\n",
    "        noise = noise.to(img.device)\n",
    "        return img + noise\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "class Clip(torch.nn.Module):\n",
    "    def __init__(self, min_val=0., max_val=1.):\n",
    "        super().__init__()\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "\n",
    "    def forward(self, img):\n",
    "        return torch.clip(img, self.min_val, self.max_val)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(min_val={0}, max_val={1})'.format(self.min_val, self.max_val)\n",
    "\n",
    "class ColorJitterPro(ColorJitter):\n",
    "    \"\"\"Randomly change the brightness, contrast, saturation, and gamma correction of an image.\"\"\"\n",
    "\n",
    "    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0, gamma=0):\n",
    "        super().__init__(brightness, contrast, saturation, hue)\n",
    "        self.gamma = self._check_input(gamma, 'gamma')\n",
    "\n",
    "    @staticmethod\n",
    "    @torch.jit.unused\n",
    "    def get_params(brightness, contrast, saturation, hue, gamma):\n",
    "        \"\"\"Get a randomized transform to be applied on image.\n",
    "\n",
    "        Arguments are same as that of __init__.\n",
    "\n",
    "        Returns:\n",
    "            Transform which randomly adjusts brightness, contrast and\n",
    "            saturation in a random order.\n",
    "        \"\"\"\n",
    "        transforms = []\n",
    "\n",
    "        if brightness is not None:\n",
    "            brightness_factor = random.uniform(brightness[0], brightness[1])\n",
    "            transforms.append(Lambda(lambda img: F.adjust_brightness(img, brightness_factor)))\n",
    "\n",
    "        if contrast is not None:\n",
    "            contrast_factor = random.uniform(contrast[0], contrast[1])\n",
    "            transforms.append(Lambda(lambda img: F.adjust_contrast(img, contrast_factor)))\n",
    "\n",
    "        if saturation is not None:\n",
    "            saturation_factor = random.uniform(saturation[0], saturation[1])\n",
    "            transforms.append(Lambda(lambda img: F.adjust_saturation(img, saturation_factor)))\n",
    "\n",
    "        if hue is not None:\n",
    "            hue_factor = random.uniform(hue[0], hue[1])\n",
    "            transforms.append(Lambda(lambda img: F.adjust_hue(img, hue_factor)))\n",
    "\n",
    "        if gamma is not None:\n",
    "            gamma_factor = random.uniform(gamma[0], gamma[1])\n",
    "            transforms.append(Lambda(lambda img: F.adjust_gamma(img, gamma_factor)))\n",
    "\n",
    "        random.shuffle(transforms)\n",
    "        transform = Compose(transforms)\n",
    "\n",
    "        return transform\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image or Tensor): Input image.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image or Tensor: Color jittered image.\n",
    "        \"\"\"\n",
    "        fn_idx = torch.randperm(5)\n",
    "        for fn_id in fn_idx:\n",
    "            if fn_id == 0 and self.brightness is not None:\n",
    "                brightness = self.brightness\n",
    "                brightness_factor = torch.tensor(1.0).uniform_(brightness[0], brightness[1]).item()\n",
    "                img = torchvisionF.adjust_brightness(img, brightness_factor)\n",
    "\n",
    "            if fn_id == 1 and self.contrast is not None:\n",
    "                contrast = self.contrast\n",
    "                contrast_factor = torch.tensor(1.0).uniform_(contrast[0], contrast[1]).item()\n",
    "                img = torchvisionF.adjust_contrast(img, contrast_factor)\n",
    "\n",
    "            if fn_id == 2 and self.saturation is not None:\n",
    "                saturation = self.saturation\n",
    "                saturation_factor = torch.tensor(1.0).uniform_(saturation[0], saturation[1]).item()\n",
    "                img = torchvisionF.adjust_saturation(img, saturation_factor)\n",
    "\n",
    "            if fn_id == 3 and self.hue is not None:\n",
    "                hue = self.hue\n",
    "                hue_factor = torch.tensor(1.0).uniform_(hue[0], hue[1]).item()\n",
    "                img = torchvisionF.adjust_hue(img, hue_factor)\n",
    "\n",
    "            if fn_id == 4 and self.gamma is not None:\n",
    "                gamma = self.gamma\n",
    "                gamma_factor = torch.tensor(1.0).uniform_(gamma[0], gamma[1]).item()\n",
    "                img = img.clamp(1e-8, 1.0)  # to fix Nan values in gradients, which happens when applying gamma\n",
    "                                            # after contrast\n",
    "                img = torchvisionF.adjust_gamma(img, gamma_factor)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        format_string += 'brightness={0}'.format(self.brightness)\n",
    "        format_string += ', contrast={0}'.format(self.contrast)\n",
    "        format_string += ', saturation={0}'.format(self.saturation)\n",
    "        format_string += ', hue={0})'.format(self.hue)\n",
    "        format_string += ', gamma={0})'.format(self.gamma)\n",
    "        return format_string\n",
    "\n",
    "def get_tta_transforms(gaussian_std: float=0.005, soft=False, clip_inputs=False):\n",
    "    img_shape = (32, 32, 3)\n",
    "    n_pixels = img_shape[0]\n",
    "\n",
    "    clip_min, clip_max = 0.0, 1.0\n",
    "\n",
    "    p_hflip = 0.5\n",
    "\n",
    "    tta_transforms = transforms.Compose([\n",
    "        Clip(0.0, 1.0), \n",
    "        ColorJitterPro(\n",
    "            brightness=[0.8, 1.2] if soft else [0.6, 1.4],\n",
    "            contrast=[0.85, 1.15] if soft else [0.7, 1.3],\n",
    "            saturation=[0.75, 1.25] if soft else [0.5, 1.5],\n",
    "            hue=[-0.03, 0.03] if soft else [-0.06, 0.06],\n",
    "            gamma=[0.85, 1.15] if soft else [0.7, 1.3]\n",
    "        ),\n",
    "        transforms.Pad(padding=int(n_pixels / 2), padding_mode='edge'),  \n",
    "        transforms.RandomAffine(\n",
    "            degrees=[-8, 8] if soft else [-15, 15],\n",
    "            translate=(1/16, 1/16),\n",
    "            scale=(0.95, 1.05) if soft else (0.9, 1.1),\n",
    "            shear=None,\n",
    "            interpolation=PIL.Image.BILINEAR,\n",
    "            fill=None\n",
    "        ),\n",
    "        transforms.GaussianBlur(kernel_size=5, sigma=[0.001, 0.25] if soft else [0.001, 0.5]),\n",
    "        transforms.CenterCrop(size=n_pixels),\n",
    "        transforms.RandomHorizontalFlip(p=p_hflip),\n",
    "        GaussianNoise(0, gaussian_std),\n",
    "        Clip(clip_min, clip_max)\n",
    "    ])\n",
    "    return tta_transforms\n",
    "\n",
    "\n",
    "def update_ema_variables(ema_model, model, alpha_teacher):\n",
    "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "        ema_param.data[:] = alpha_teacher * ema_param[:].data[:] + (1 - alpha_teacher) * param[:].data[:]\n",
    "    return ema_model\n",
    "\n",
    "\n",
    "class CoTTA(nn.Module):\n",
    "    \"\"\"CoTTA adapts a model by entropy minimization during testing.\n",
    "\n",
    "    Once tented, a model adapts itself by updating on every forward.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, optimizer, steps=1, episodic=False, mt_alpha=0.99, rst_m=0.1, ap=0.9):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.steps = steps\n",
    "        assert steps > 0, \"cotta requires >= 1 step(s) to forward and update\"\n",
    "        self.episodic = episodic\n",
    "        \n",
    "        self.model_state, self.optimizer_state, self.model_ema, self.model_anchor = \\\n",
    "            copy_model_and_optimizer(self.model, self.optimizer)\n",
    "        self.transform = get_tta_transforms()    \n",
    "        self.mt = mt_alpha\n",
    "        self.rst = rst_m\n",
    "        self.ap = ap\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.episodic:\n",
    "            self.reset()\n",
    "\n",
    "        for _ in range(self.steps):\n",
    "            outputs = self.forward_and_adapt(x, self.model, self.optimizer)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def reset(self):\n",
    "        if self.model_state is None or self.optimizer_state is None:\n",
    "            raise Exception(\"cannot reset without saved model/optimizer state\")\n",
    "        load_model_and_optimizer(self.model, self.optimizer,\n",
    "                                 self.model_state, self.optimizer_state)\n",
    "        # Use this line to also restore the teacher model                         \n",
    "        self.model_state, self.optimizer_state, self.model_ema, self.model_anchor = \\\n",
    "            copy_model_and_optimizer(self.model, self.optimizer)\n",
    "\n",
    "\n",
    "    @torch.enable_grad()  # ensure grads in possible no grad context for testing\n",
    "    def forward_and_adapt(self, x, model, optimizer):\n",
    "        outputs = self.model(x)\n",
    "        # Teacher Prediction\n",
    "        anchor_prob = torch.nn.functional.softmax(self.model_anchor(x), dim=1).max(1)[0]\n",
    "        standard_ema = self.model_ema(x)\n",
    "        # Augmentation-averaged Prediction\n",
    "        N = 32 \n",
    "        outputs_emas = []\n",
    "        for i in range(N):\n",
    "            outputs_  = self.model_ema(self.transform(x)).detach()\n",
    "            outputs_emas.append(outputs_)\n",
    "        # Threshold choice discussed in supplementary\n",
    "        if anchor_prob.mean(0)<self.ap:\n",
    "            outputs_ema = torch.stack(outputs_emas).mean(0)\n",
    "        else:\n",
    "            outputs_ema = standard_ema\n",
    "        # Student update\n",
    "        loss = (softmax_entropy(outputs, outputs_ema)).mean(0) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Teacher update\n",
    "        self.model_ema = update_ema_variables(ema_model = self.model_ema, model = self.model, alpha_teacher=self.mt)\n",
    "        # Stochastic restore\n",
    "        if True:\n",
    "            for nm, m  in self.model.named_modules():\n",
    "                for npp, p in m.named_parameters():\n",
    "                    if npp in ['weight', 'bias'] and p.requires_grad:\n",
    "                        mask = (torch.rand(p.shape)<self.rst).float()\n",
    "                        with torch.no_grad():\n",
    "                            p.data = self.model_state[f\"{nm}.{npp}\"] * mask + p * (1.-mask)\n",
    "        return outputs_ema\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def softmax_entropy(x, x_ema):# -> torch.Tensor:\n",
    "    \"\"\"Entropy of softmax distribution from logits.\"\"\"\n",
    "    return -(x_ema.softmax(1) * x.log_softmax(1)).sum(1)\n",
    "\n",
    "def cotta_collect_params(model):\n",
    "    \"\"\"Collect all trainable parameters.\n",
    "\n",
    "    Walk the model's modules and collect all parameters.\n",
    "    Return the parameters and their names.\n",
    "\n",
    "    Note: other choices of parameterization are possible!\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    names = []\n",
    "    for nm, m in model.named_modules():\n",
    "        if True:#isinstance(m, nn.BatchNorm2d): collect all \n",
    "            for np, p in m.named_parameters():\n",
    "                if np in ['weight', 'bias'] and p.requires_grad:\n",
    "                    params.append(p)\n",
    "                    names.append(f\"{nm}.{np}\")\n",
    "    return params, names\n",
    "\n",
    "\n",
    "def copy_model_and_optimizer(model, optimizer):\n",
    "    \"\"\"Copy the model and optimizer states for resetting after adaptation.\"\"\"\n",
    "    model_state = deepcopy(model.state_dict())\n",
    "    model_anchor = deepcopy(model)\n",
    "    optimizer_state = deepcopy(optimizer.state_dict())\n",
    "    ema_model = deepcopy(model)\n",
    "    for param in ema_model.parameters():\n",
    "        param.detach_()\n",
    "    return model_state, optimizer_state, ema_model, model_anchor\n",
    "\n",
    "\n",
    "def load_model_and_optimizer(model, optimizer, model_state, optimizer_state):\n",
    "    \"\"\"Restore the model and optimizer states from copies.\"\"\"\n",
    "    model.load_state_dict(model_state, strict=True)\n",
    "    optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "\n",
    "def cotta_configure_model(model):\n",
    "    \"\"\"Configure model for use with tent.\"\"\"\n",
    "    # train mode, because tent optimizes the model to minimize entropy\n",
    "    model.train()\n",
    "    # disable grad, to (re-)enable only what we update\n",
    "    model.requires_grad_(False)\n",
    "    # enable all trainable\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            m.requires_grad_(True)\n",
    "            # force use of batch stats in train and eval modes\n",
    "            m.track_running_stats = False\n",
    "            m.running_mean = None\n",
    "            m.running_var = None\n",
    "        else:\n",
    "            m.requires_grad_(True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TENT (Test-time Adaptation by Entropy Minimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Builds upon: https://github.com/qinenergy/cotta\n",
    "Corresponding paper: https://arxiv.org/abs/2006.10726\n",
    "\"\"\"\n",
    "\n",
    "class Tent(nn.Module):\n",
    "    \"\"\"\n",
    "    Tent adapts a model by entropy minimization during testing.\n",
    "    Once tented, a model adapts itself by updating on every forward.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, optimizer, steps = 1):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.steps = steps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for _ in range(self.steps):\n",
    "            outputs = tent_forward_and_adapt(x, self.model, self.optimizer)\n",
    "        return outputs\n",
    "\n",
    "@torch.jit.script\n",
    "def tent_softmax_entropy(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Entropy of softmax distribution from logits.\"\"\"\n",
    "    return -(x.softmax(1) * x.log_softmax(1)).sum(1)\n",
    "\n",
    "\n",
    "@torch.enable_grad()  # ensure grads in possible no grad context for testing\n",
    "def tent_forward_and_adapt(x, model, optimizer):\n",
    "    \"\"\"Forward and adapt model on batch of data.\n",
    "\n",
    "    Measure entropy of the model prediction, take gradients, and update params.\n",
    "    \"\"\"\n",
    "    # forward\n",
    "    outputs = model(x)\n",
    "    # adapt\n",
    "    loss = tent_softmax_entropy(outputs).mean(0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return outputs\n",
    "\n",
    "def tent_configure_model(model):\n",
    "    \"\"\"Configure model for use with tent.\"\"\"\n",
    "    # train mode, because tent optimizes the model to minimize entropy\n",
    "    model.train()\n",
    "    # disable grad, to (re-)enable only what tent updates\n",
    "    model.requires_grad_(False)\n",
    "    # configure norm for tent updates: enable grad + force batch statisics\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            m.requires_grad_(True)\n",
    "            # force use of batch stats in train and eval modes\n",
    "            m.track_running_stats = False\n",
    "            m.running_mean = None\n",
    "            m.running_var = None\n",
    "    return model\n",
    "\n",
    "def tent_collect_params(model):\n",
    "    \"\"\"Collect the affine scale + shift parameters from batch norms.\n",
    "\n",
    "    Walk the model's modules and collect all batch normalization parameters.\n",
    "    Return the parameters and their names.\n",
    "\n",
    "    Note: other choices of parameterization are possible!\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    names = []\n",
    "    for nm, m in model.named_modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            for np, p in m.named_parameters():\n",
    "                if np in ['weight', 'bias']:  # weight is scale, bias is shift\n",
    "                    params.append(p)\n",
    "                    names.append(f\"{nm}.{np}\")\n",
    "    return params, names\n",
    "\n",
    "def tent_check_model(model):\n",
    "    \"\"\"Check model for compatability with tent.\"\"\"\n",
    "    is_training = model.training\n",
    "    assert is_training, \"tent needs train mode: call model.train()\"\n",
    "    param_grads = [p.requires_grad for p in model.parameters()]\n",
    "    has_any_params = any(param_grads)\n",
    "    has_all_params = all(param_grads)\n",
    "    assert has_any_params, \"tent needs params to update: \" \\\n",
    "                           \"check which require grad\"\n",
    "    assert not has_all_params, \"tent should not update all params: \" \\\n",
    "                               \"check which require grad\"\n",
    "    has_bn = any([isinstance(m, nn.BatchNorm2d) for m in model.modules()])\n",
    "    assert has_bn, \"tent needs normalization for its optimization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_raw(model):\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def setup_tent(model, steps = 1):\n",
    "    \"\"\"\n",
    "    Set up tent adaptation.\n",
    "    \"\"\"\n",
    "    model = tent_configure_model(model)\n",
    "    params, param_names = tent_collect_params(model)\n",
    "    optimizer = optim.Adam(params = params)\n",
    "    tent_model = Tent(model = model, optimizer = optimizer, steps = steps)\n",
    "    return tent_model\n",
    "\n",
    "def setup_cotta(model, steps = 1):\n",
    "    \"\"\"\n",
    "    Set up CoTTA adaptation.\n",
    "    \"\"\"\n",
    "    model = cotta_configure_model(model)\n",
    "    params, param_names = cotta_collect_params(model)\n",
    "    optimizer = optim.Adam(params = params, lr = 0.002)\n",
    "    cotta_model = CoTTA(model = model, optimizer = optimizer, steps = steps)\n",
    "    return cotta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/Tiny-ImageNet-C\\zoom_blur\\4\n"
     ]
    }
   ],
   "source": [
    "imbalanced_data_folder = imbalanced_data_folders[random.randint(len(imbalanced_data_folders))]\n",
    "index = random.randint(5) + 1\n",
    "dir_corrupt = os.path.join(dir_data, imbalanced_data_folder, str(index))\n",
    "print(dir_corrupt)\n",
    "\n",
    "def evaluate_tiny_imagenet_C(model, batch_size = 100):\n",
    "    # evaluate on each severity and type of corruption in turn\n",
    "    ground_truth = []\n",
    "    inputs = torch.tensor([])\n",
    "    total_correct = 0\n",
    "    total_data = 0\n",
    "\n",
    "    dir_classes = os.listdir(dir_corrupt)\n",
    "    for dir_class in tqdm(dir_classes):\n",
    "        data_folder = os.path.join(dir_corrupt, dir_class)\n",
    "        label = mapping[dir_class]\n",
    "        images = os.listdir(data_folder)\n",
    "        for image in images:\n",
    "            dir_image = os.path.join(data_folder, image)\n",
    "            ground_truth.append(label)\n",
    "\n",
    "            image = Image.open(dir_image)\n",
    "            input = preprocess(image).unsqueeze(0)\n",
    "            if input.numel() == 0:\n",
    "                inputs = input\n",
    "            else:\n",
    "                inputs = torch.cat((inputs, input))\n",
    "\n",
    "            if inputs.shape[0] == batch_size:\n",
    "                outputs = model(inputs)\n",
    "                outputs = torch.argmax(outputs, axis = 1).detach().cpu().numpy()\n",
    "                accuracy = get_accuracy(outputs, ground_truth)\n",
    "                total_correct += accuracy * batch_size\n",
    "                total_data += batch_size\n",
    "\n",
    "                inputs = torch.tensor([])\n",
    "                ground_truth = []\n",
    "    \n",
    "    if inputs.shape[0] > 0:\n",
    "        outputs = model(inputs)\n",
    "        outputs = torch.argmax(outputs, axis = 1).detach().cpu().numpy()\n",
    "        accuracy = get_accuracy(outputs, ground_truth)\n",
    "        total_correct += accuracy * images.shape[0]\n",
    "        total_data += images.shape[0]\n",
    "\n",
    "    return total_correct / total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [04:59<00:00,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw_model = setup_raw(model1)\n",
    "accuracy = evaluate_tiny_imagenet_C(raw_model)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [05:36<00:00,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tent_model = setup_tent(model2)\n",
    "tent_check_model(tent_model)\n",
    "accuracy = evaluate_tiny_imagenet_C(tent_model)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [1:01:18<00:00, 18.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cotta_model = setup_cotta(model3)\n",
    "accuracy = evaluate_tiny_imagenet_C(cotta_model)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
